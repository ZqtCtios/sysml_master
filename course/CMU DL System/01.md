# ML Refresher / Softmax Regression
[https://www.youtube.com/watch?v=MlivXhZFbNA&ab_channel=DeepLearningSystemsCourse](https://www.youtube.com/watch?v=MlivXhZFbNA&ab_channel=DeepLearningSystemsCourse
)
## Basics of mechine learning

Machine Learning as data-driven programming

Traning Data -> Machine Learning Algorihtm -> Model h

1. The hypothesis class
2. The loss function
3. An optimization method

## Softmax Regression

a k-class classification setting

* Training Data:  $x^{(i)} \in \mathbb{R}^n, y^{(i)} \in\{1, \ldots, k\}$ for $i=1, \ldots m$
* $n$ = dim of input data
* $k$ = num of labels
* $m$ = num of points in the training set

Example: classification of 28X28 MNIST digits

* $n=28*28=784$
* $k=10$
* $m=60000$

Our hypothesis function maps inputs $x \in \mathbb{R}^n$ to $k$-dimensional vectors
$$
h: \mathbb{R}^n \rightarrow \mathbb{R}^k
$$
where $h_i(x)$ indicates some measure of "belief" in how much likely the label is to be class $i$ (i.e., "most likely" prediction is coordinate $i$ with largest $h_i(x)$ ).

A linear hypothesis function uses a linear operator (i.e. matrix multiplication) for this transformation
$$
h_\theta(x)=\theta^T x
$$
for parameters $\theta \in \mathbb{R}^{n \times k}$

Often more convenient (and this is how you want to code things for efficiency) to write the data and operations in matrix batch form
$$
X \in \mathbb{R}^{m \times n}=\left[\begin{array}{c}
-x^{(1)^T}- \\
\vdots \\
-x^{(m)^T}-
\end{array}\right], \quad y \in\{1, \ldots, k\}^m=\left[\begin{array}{c}
y^{(1)} \\
\vdots \\
y^{(m)}
\end{array}\right]
$$
Then the linear hypothesis applied to this batch can be written as
$$
h_\theta(X)=\left[\begin{array}{c}
-h_\theta\left(x^{(1)}\right)^T- \\
\vdots \\
-h_\theta\left(x^m\right)^T-
\end{array}\right]
$$